{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Import the data: You are provided separate .csv files for train and test.\n",
    "\n",
    "Train shape: (507, 148)\n",
    "Test shape: (168, 148)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (507, 148)\n",
      "Test shape: (168, 148)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "test  = pd.read_csv('test_data.csv')\n",
    "print('Train shape:',train.shape)\n",
    "print('Test shape:',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>BrdIndx</th>\n",
       "      <th>Area</th>\n",
       "      <th>Round</th>\n",
       "      <th>Bright</th>\n",
       "      <th>Compact</th>\n",
       "      <th>ShpIndx</th>\n",
       "      <th>Mean_G</th>\n",
       "      <th>Mean_R</th>\n",
       "      <th>Mean_NIR</th>\n",
       "      <th>...</th>\n",
       "      <th>SD_NIR_140</th>\n",
       "      <th>LW_140</th>\n",
       "      <th>GLCM1_140</th>\n",
       "      <th>Rect_140</th>\n",
       "      <th>GLCM2_140</th>\n",
       "      <th>Dens_140</th>\n",
       "      <th>Assym_140</th>\n",
       "      <th>NDVI_140</th>\n",
       "      <th>BordLngth_140</th>\n",
       "      <th>GLCM3_140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>concrete</td>\n",
       "      <td>1.32</td>\n",
       "      <td>131</td>\n",
       "      <td>0.81</td>\n",
       "      <td>222.74</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.18</td>\n",
       "      <td>192.94</td>\n",
       "      <td>235.11</td>\n",
       "      <td>240.15</td>\n",
       "      <td>...</td>\n",
       "      <td>31.15</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1512</td>\n",
       "      <td>1287.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shadow</td>\n",
       "      <td>1.59</td>\n",
       "      <td>864</td>\n",
       "      <td>0.94</td>\n",
       "      <td>47.56</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.87</td>\n",
       "      <td>36.82</td>\n",
       "      <td>48.78</td>\n",
       "      <td>57.09</td>\n",
       "      <td>...</td>\n",
       "      <td>12.01</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.96</td>\n",
       "      <td>7.01</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>196</td>\n",
       "      <td>2659.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shadow</td>\n",
       "      <td>1.41</td>\n",
       "      <td>409</td>\n",
       "      <td>1.00</td>\n",
       "      <td>51.38</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.53</td>\n",
       "      <td>41.72</td>\n",
       "      <td>51.96</td>\n",
       "      <td>60.48</td>\n",
       "      <td>...</td>\n",
       "      <td>18.75</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.63</td>\n",
       "      <td>8.32</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1198</td>\n",
       "      <td>720.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tree</td>\n",
       "      <td>2.58</td>\n",
       "      <td>187</td>\n",
       "      <td>1.91</td>\n",
       "      <td>70.08</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.11</td>\n",
       "      <td>93.13</td>\n",
       "      <td>55.20</td>\n",
       "      <td>61.92</td>\n",
       "      <td>...</td>\n",
       "      <td>27.67</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8.56</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.20</td>\n",
       "      <td>524</td>\n",
       "      <td>891.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asphalt</td>\n",
       "      <td>2.60</td>\n",
       "      <td>116</td>\n",
       "      <td>2.05</td>\n",
       "      <td>89.57</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.02</td>\n",
       "      <td>73.17</td>\n",
       "      <td>94.89</td>\n",
       "      <td>100.64</td>\n",
       "      <td>...</td>\n",
       "      <td>32.05</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8.62</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>496</td>\n",
       "      <td>1194.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class  BrdIndx  Area  Round  Bright  Compact  ShpIndx  Mean_G  Mean_R  \\\n",
       "0  concrete      1.32   131   0.81  222.74     1.66     2.18  192.94  235.11   \n",
       "1    shadow      1.59   864   0.94   47.56     1.41     1.87   36.82   48.78   \n",
       "2    shadow      1.41   409   1.00   51.38     1.37     1.53   41.72   51.96   \n",
       "3      tree      2.58   187   1.91   70.08     3.41     3.11   93.13   55.20   \n",
       "4   asphalt      2.60   116   2.05   89.57     3.06     3.02   73.17   94.89   \n",
       "\n",
       "   Mean_NIR  ...  SD_NIR_140  LW_140  GLCM1_140  Rect_140  GLCM2_140  \\\n",
       "0    240.15  ...       31.15    5.04       0.80      0.58       8.56   \n",
       "1     57.09  ...       12.01    3.70       0.52      0.96       7.01   \n",
       "2     60.48  ...       18.75    3.09       0.90      0.63       8.32   \n",
       "3     61.92  ...       27.67    6.33       0.89      0.70       8.56   \n",
       "4    100.64  ...       32.05    1.01       0.83      0.75       8.62   \n",
       "\n",
       "   Dens_140  Assym_140  NDVI_140  BordLngth_140  GLCM3_140  \n",
       "0      0.82       0.98     -0.10           1512    1287.52  \n",
       "1      1.69       0.86     -0.14            196    2659.74  \n",
       "2      1.38       0.84      0.10           1198     720.38  \n",
       "3      1.10       0.96      0.20            524     891.36  \n",
       "4      2.08       0.08     -0.10            496    1194.76  \n",
       "\n",
       "[5 rows x 148 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Remove any rows that have missing data across both sets of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class            0\n",
      "BrdIndx          0\n",
      "Area             0\n",
      "Round            0\n",
      "Bright           0\n",
      "                ..\n",
      "Dens_140         0\n",
      "Assym_140        0\n",
      "NDVI_140         0\n",
      "BordLngth_140    0\n",
      "GLCM3_140        0\n",
      "Length: 148, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class            0\n",
      "BrdIndx          0\n",
      "Area             0\n",
      "Round            0\n",
      "Bright           0\n",
      "                ..\n",
      "Dens_140         0\n",
      "Assym_140        0\n",
      "NDVI_140         0\n",
      "BordLngth_140    0\n",
      "GLCM3_140        0\n",
      "Length: 148, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above missing value report we can conclude that there are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### c) The target variable (dependent variable) is called \"class\", make sure to separate this out into a \"y_train\" and \"y_test\" and remove from your \"X_train\" and \"X_test\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating the target variable and predictors\n",
    "y_train = train.iloc[:,0]\n",
    "X_train = train.iloc[:,1:]\n",
    "y_test  = test.iloc[:,0]\n",
    "X_test  = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Scale all features / predictors (NOT THE TARGET VARIABLE)\n",
    "Feel free to use the sklearn tool \"StandardScaler\" - more info here: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html (Links to an external site.)\n",
    "Note: We need to scale here due to SVM. Please refer to previous assignments if you have forgotten appropriate scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the X_train and X_test\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "scaler.fit(X_test)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest Classifier - Base Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by creating a simple Random Forest only using default parameters - this will let us compare SVMs to Random Forest in multiclass problems.\n",
    "#### a) Use the RandomForestClassifier in sklearn. Fit your model on the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a random forests classifer\n",
    "rf = RandomForestClassifier(random_state=20,class_weight='balanced')\n",
    "# Fitting the data to the model\n",
    "rf.fit(X_train_scaled, y_train,clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### b) Use the fitted model to predict on test data. Use the .predict() method to get the predicted classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the test data based on the trained model\n",
    "y_pred = rf.predict(X_test_scaled,clas)\n",
    "\n",
    "#predicting probabilities to calculate roc_auc scores\n",
    "y_test_pred_prob = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Calculate the confusion matrix and classification report for the test data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 19,  0,  4,  1,  0,  0,  0,  0],\n",
       "       [ 0,  1, 14,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  2,  0, 20,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0, 24,  0,  0,  0,  5],\n",
       "       [ 1,  0,  1,  0,  0, 13,  0,  0,  0],\n",
       "       [ 2,  0,  0,  0,  0,  0, 14,  0,  0],\n",
       "       [ 0,  1,  0,  5,  2,  0,  0,  6,  0],\n",
       "       [ 0,  0,  0,  1,  1,  0,  0,  0, 15]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.78      1.00      0.88        14\n",
      "   building        0.83      0.76      0.79        25\n",
      "        car        0.93      0.93      0.93        15\n",
      "   concrete        0.67      0.87      0.75        23\n",
      "      grass        0.86      0.83      0.84        29\n",
      "       pool        1.00      0.87      0.93        15\n",
      "     shadow        1.00      0.88      0.93        16\n",
      "       soil        0.86      0.43      0.57        14\n",
      "       tree        0.75      0.88      0.81        17\n",
      "\n",
      "    accuracy                           0.83       168\n",
      "   macro avg       0.85      0.83      0.83       168\n",
      "weighted avg       0.84      0.83      0.82       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### d)  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Confusion Matrix\n",
      "[[45  0  0  0  0  0  0  0  0]\n",
      " [ 0 97  0  0  0  0  0  0  0]\n",
      " [ 0  0 21  0  0  0  0  0  0]\n",
      " [ 0  0  0 93  0  0  0  0  0]\n",
      " [ 0  0  0  0 83  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  0  0  0  0  0  0 89]]\n",
      "\n",
      "\n",
      "\t\t\t Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      1.00      1.00        45\n",
      "   building        1.00      1.00      1.00        97\n",
      "        car        1.00      1.00      1.00        21\n",
      "   concrete        1.00      1.00      1.00        93\n",
      "      grass        1.00      1.00      1.00        83\n",
      "       pool        1.00      1.00      1.00        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      1.00      1.00        20\n",
      "       tree        1.00      1.00      1.00        89\n",
      "\n",
      "    accuracy                           1.00       507\n",
      "   macro avg       1.00      1.00      1.00       507\n",
      "weighted avg       1.00      1.00      1.00       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predicting the classes based on train data\n",
    "y_train_pred = rf.predict(X_train_scaled)\n",
    "\n",
    "#confusion matrix\n",
    "print(\"\\t\\t\\t Confusion Matrix\")\n",
    "print(confusion_matrix(y_train,y_train_pred))\n",
    "print('\\n')\n",
    "\n",
    "print(\"\\t\\t\\t Classification report\")\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification report of the train data we can observe that the model has generalized the data to the maximum extent and was able to predict the train data with 100% accuracy. So, based on the above metrics and the test metrics we can conclude that the **random forest with default parameters is overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Identify the top 5 features. Feel free to print a list OR to make a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00283965, 0.00708813, 0.00191604, 0.01351364, 0.00251535,\n",
       "       0.00180304, 0.02001479, 0.02601936, 0.02816106, 0.00556653])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the important features deemed by the model\n",
    "rf.feature_importances_[:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BrdIndx', 'Area', 'Round', 'Bright', 'Compact', 'ShpIndx', 'Mean_G',\n",
       "       'Mean_R', 'Mean_NIR', 'SD_G'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning the column names\n",
    "features = X_train.columns[:10,]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Barplot to plot the features according to the importance\n",
    "plt.figure(figsize=(10,10))\n",
    "importance = rf.feature_importances_[:10,]\n",
    "indices = np.argsort(importance)\n",
    "plt.title('Variable feature importance')\n",
    "plt.barh(range(len(indices)), importance[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the feature importance plot, the top 5 features are **Mean_NIR, Mean_R, Mean_G, Bright, Area**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LinearSVM Classifier - Base Model:\n",
    "Create a simple LinearSVC Classifier only using default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### a) Use the LinearSVC in sklearn. Fit your model on the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='linear')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next lets try Linear SVM\n",
    "svc = svm.SVC(kernel='linear', C=1)\n",
    "#Fitting the data to the model\n",
    "svc.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Use the fitted model to predict on test data. Use the .predict() method to get the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Calculate the confusion matrix and classification report for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Confusion Matrix\n",
      "[[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 22  1  2  0  0  0  0  0]\n",
      " [ 0  0 13  1  0  0  0  1  0]\n",
      " [ 0  4  0 16  0  0  0  3  0]\n",
      " [ 0  0  0  1 22  0  0  0  6]\n",
      " [ 0  2  0  0  0 12  1  0  0]\n",
      " [ 1  0  0  0  0  0 15  0  0]\n",
      " [ 0  0  1  4  3  0  0  6  0]\n",
      " [ 0  0  0  1  1  0  0  0 15]]\n",
      "\n",
      "\n",
      "\t\t\t Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.93      0.93        14\n",
      "   building        0.79      0.88      0.83        25\n",
      "        car        0.87      0.87      0.87        15\n",
      "   concrete        0.64      0.70      0.67        23\n",
      "      grass        0.85      0.76      0.80        29\n",
      "       pool        1.00      0.80      0.89        15\n",
      "     shadow        0.88      0.94      0.91        16\n",
      "       soil        0.60      0.43      0.50        14\n",
      "       tree        0.71      0.88      0.79        17\n",
      "\n",
      "    accuracy                           0.80       168\n",
      "   macro avg       0.81      0.80      0.80       168\n",
      "weighted avg       0.80      0.80      0.80       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "print(\"\\t\\t\\t Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "\n",
    "print(\"\\t\\t\\t Classification report\")\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **precision** of the linear SVM model is **81%** and the **accuracy** is **80%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Confusion Matrix\n",
      "[[45  0  0  0  0  0  0  0  0]\n",
      " [ 0 97  0  0  0  0  0  0  0]\n",
      " [ 0  0 21  0  0  0  0  0  0]\n",
      " [ 0  0  0 93  0  0  0  0  0]\n",
      " [ 0  0  0  0 83  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  0  0  0  0  0  0 89]]\n",
      "\n",
      "\n",
      "\t\t\t Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      1.00      1.00        45\n",
      "   building        1.00      1.00      1.00        97\n",
      "        car        1.00      1.00      1.00        21\n",
      "   concrete        1.00      1.00      1.00        93\n",
      "      grass        1.00      1.00      1.00        83\n",
      "       pool        1.00      1.00      1.00        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      1.00      1.00        20\n",
      "       tree        1.00      1.00      1.00        89\n",
      "\n",
      "    accuracy                           1.00       507\n",
      "   macro avg       1.00      1.00      1.00       507\n",
      "weighted avg       1.00      1.00      1.00       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predicting the classes based on train data\n",
    "y_train_pred = svc.predict(X_train_scaled)\n",
    "\n",
    "#confusion matrix\n",
    "print(\"\\t\\t\\t Confusion Matrix\")\n",
    "print(confusion_matrix(y_train,y_train_pred))\n",
    "print('\\n')\n",
    "\n",
    "print(\"\\t\\t\\t Classification report\")\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification report of the train data we can observe that the model has generalized the data to the maximum extent and was able to predict the train data with **100% accuracy**. So, based on the above metrics and the test metrics we can conclude that the random forest with default parameters is **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machine Classifier + Linear Kernel + Grid Search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use GridSearchCV to try various hyperparameters in a SVM with linear kernel.\n",
    "\n",
    "### a) Use SVC from sklearn with kernel = \"linear\". Run the GridSearchCV using the following (SVMs run much faster than RandomForest):\n",
    "\n",
    "C: 0.01 - 10 in increments of 0.2 (consider using the np.arange() method from numpy to build out a sequence of values)\n",
    "\n",
    "Note: Feel free to try out more parameters, the above is the bare minimum for this assignment.\n",
    "\n",
    "Use 5 cross-fold and the default scoring. Please set verbose = 0 to reduce the printing (sorry to our grader for not specifying this last week!).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': np.arange(0.01, 10, 0.2).tolist(),\n",
    "              'kernel': ['linear']} \n",
    "\n",
    "# create AdaBoost Classifier model \n",
    "svc = svm.SVC()\n",
    "\n",
    "# Use 5 cross-fold and for scoring use \"roc_auc\" \n",
    "svc_Grid = GridSearchCV(svc, param_grid, cv = 5,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.01, 0.21000000000000002, 0.41000000000000003,\n",
       "                               0.6100000000000001, 0.81, 1.01,\n",
       "                               1.2100000000000002, 1.4100000000000001, 1.61,\n",
       "                               1.81, 2.01, 2.21, 2.41, 2.61, 2.81, 3.01, 3.21,\n",
       "                               3.41, 3.61, 3.81, 4.01, 4.21, 4.41, 4.61,\n",
       "                               4.8100000000000005, 5.01, 5.21, 5.41, 5.61,\n",
       "                               5.8100000000000005, ...],\n",
       "                         'kernel': ['linear']})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit our model to our train data\n",
    "svc_Grid.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Identify the best performing model:\n",
    "\n",
    ".best_params_() : This method outputs to best performing parameters  \n",
    ".best_estimator_() : This method outputs the best performing model, and can be used for predicting on the X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best performing parameters\n",
    "svc_Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.01, kernel='linear')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best performing model\n",
    "svc_Grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Use the best estimator model to predict on test data. Use the .predict() method to get the predicted classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the best estimator model\n",
    "svc_best = svm.SVC(C=0.01, kernel='linear')\n",
    "\n",
    "#Fitting the best fit model on train data\n",
    "svc_best.fit(X_train_scaled,y_train)\n",
    "\n",
    "#predicting the classes of test data\n",
    "y_pred = svc_best.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Calculate the confusion matrix and classification report for test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Confusion Matrix\n",
      "[[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 22  0  2  1  0  0  0  0]\n",
      " [ 0  1 14  0  0  0  0  0  0]\n",
      " [ 0  3  0 19  0  0  0  1  0]\n",
      " [ 0  0  0  1 25  0  0  0  3]\n",
      " [ 0  1  0  0  0 13  1  0  0]\n",
      " [ 3  0  0  0  0  0 13  0  0]\n",
      " [ 0  1  0  6  3  0  0  4  0]\n",
      " [ 0  0  0  1  0  0  0  0 16]]\n",
      "\n",
      "\n",
      "\t\t\t Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.81      0.93      0.87        14\n",
      "   building        0.79      0.88      0.83        25\n",
      "        car        1.00      0.93      0.97        15\n",
      "   concrete        0.66      0.83      0.73        23\n",
      "      grass        0.86      0.86      0.86        29\n",
      "       pool        1.00      0.87      0.93        15\n",
      "     shadow        0.87      0.81      0.84        16\n",
      "       soil        0.80      0.29      0.42        14\n",
      "       tree        0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.83       168\n",
      "   macro avg       0.85      0.82      0.81       168\n",
      "weighted avg       0.84      0.83      0.82       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "print(\"\\t\\t\\t Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "\n",
    "print(\"\\t\\t\\t Classification report\")\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **precision** of the linear SVM model is **85%** and the **accuracy** is **83%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Confusion Matrix\n",
      "[[40  0  0  0  0  0  5  0  0]\n",
      " [ 2 87  0  7  0  0  1  0  0]\n",
      " [ 0  1 19  1  0  0  0  0  0]\n",
      " [ 0  9  0 83  1  0  0  0  0]\n",
      " [ 0  1  0  0 70  0  0  0 12]\n",
      " [ 0  1  0  0  1 12  0  0  0]\n",
      " [ 1  0  0  0  0  0 43  0  1]\n",
      " [ 0  3  0  4  2  0  0 11  0]\n",
      " [ 0  0  0  0  3  0  1  0 85]]\n",
      "\n",
      "\n",
      "\t\t\t Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.89      0.91        45\n",
      "   building        0.85      0.90      0.87        97\n",
      "        car        1.00      0.90      0.95        21\n",
      "   concrete        0.87      0.89      0.88        93\n",
      "      grass        0.91      0.84      0.88        83\n",
      "       pool        1.00      0.86      0.92        14\n",
      "     shadow        0.86      0.96      0.91        45\n",
      "       soil        1.00      0.55      0.71        20\n",
      "       tree        0.87      0.96      0.91        89\n",
      "\n",
      "    accuracy                           0.89       507\n",
      "   macro avg       0.92      0.86      0.88       507\n",
      "weighted avg       0.89      0.89      0.89       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predicting the classes based on train data\n",
    "y_train_pred = svc_best.predict(X_train_scaled)\n",
    "\n",
    "#confusion matrix\n",
    "print(\"\\t\\t\\t Confusion Matrix\")\n",
    "print(confusion_matrix(y_train,y_train_pred))\n",
    "print('\\n')\n",
    "\n",
    "print(\"\\t\\t\\t Classification report\")\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification report of the train data we can observe that the model is **not over fitting**.\n",
    "\n",
    "One of the conditions for overfitting is that the train error is too large than the test error. In this Linear SVM classifier with GridSearchCV the training error is greater than the test error, however, it's not that large to be termed as overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Support Vector Machine Classifier + Polynomial Kernel + Grid Search:\n",
    "\n",
    "We will now use GridSearchCV to try various hyperparameters in a SVM with a polynomial kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### a) Use SVC from sklearn with kernel = \"poly\". Run the GridSearchCV using the following:\n",
    "\n",
    "C: 0.01 - 10 in increments of 0.2  \n",
    "degree: 2, 3, 4, 5, 6\n",
    "\n",
    "Note: Feel free to try out more parameters, the above is the bare minimum for this assignment.\n",
    "\n",
    "Use 5 cross-fold and the default scoring.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': np.arange(0.01, 10, 0.2).tolist(),\n",
    "              'kernel': ['poly'],\n",
    "             'degree' : [2,3,4,5,6]} \n",
    "\n",
    "# create AdaBoost Classifier model \n",
    "svc = svm.SVC()\n",
    "\n",
    "# Use 5 cross-fold and for scoring use \"roc_auc\" \n",
    "svc_Grid = GridSearchCV(svc, param_grid, cv = 5,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.01, 0.21000000000000002, 0.41000000000000003,\n",
       "                               0.6100000000000001, 0.81, 1.01,\n",
       "                               1.2100000000000002, 1.4100000000000001, 1.61,\n",
       "                               1.81, 2.01, 2.21, 2.41, 2.61, 2.81, 3.01, 3.21,\n",
       "                               3.41, 3.61, 3.81, 4.01, 4.21, 4.41, 4.61,\n",
       "                               4.8100000000000005, 5.01, 5.21, 5.41, 5.61,\n",
       "                               5.8100000000000005, ...],\n",
       "                         'degree': [2, 3, 4, 5, 6], 'kernel': ['poly']})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit our model to our train data\n",
    "svc_Grid.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Identify the best performing model:\n",
    "\n",
    ".best_params_() : This method outputs to best performing parameters\n",
    ".best_estimator_() : This method outputs the best performing model, and can be used for predicting on the X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 3.81, 'degree': 3, 'kernel': 'poly'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best performing parameters\n",
    "svc_Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=3.81, kernel='poly')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best performing model\n",
    "svc_Grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Use the best estimator model to predict on test data. Use the .predict() method to get the predicted classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the best estimator model\n",
    "svc_best = svm.SVC(C=3.81, kernel='poly', degree =3 )\n",
    "\n",
    "#Fitting the best fit model on train data\n",
    "svc_best.fit(X_train_scaled,y_train)\n",
    "\n",
    "#predicting the classes of test data\n",
    "y_pred = svc_best.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Calculate the confusion matrix and classification report for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Confusion Matrix\n",
      "[[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 18  0  4  3  0  0  0  0]\n",
      " [ 0  2 11  0  0  1  0  1  0]\n",
      " [ 0  3  0 19  1  0  0  0  0]\n",
      " [ 0  0  0  0 26  0  0  1  2]\n",
      " [ 0  4  0  0  0 10  1  0  0]\n",
      " [ 1  0  0  0  0  0 14  0  1]\n",
      " [ 0  1  0  5  8  0  0  0  0]\n",
      " [ 0  0  0  1  3  0  0  0 13]]\n",
      "\n",
      "\n",
      "\t\t\t Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.93      0.93        14\n",
      "   building        0.64      0.72      0.68        25\n",
      "        car        1.00      0.73      0.85        15\n",
      "   concrete        0.66      0.83      0.73        23\n",
      "      grass        0.63      0.90      0.74        29\n",
      "       pool        0.91      0.67      0.77        15\n",
      "     shadow        0.88      0.88      0.88        16\n",
      "       soil        0.00      0.00      0.00        14\n",
      "       tree        0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.74       168\n",
      "   macro avg       0.72      0.71      0.71       168\n",
      "weighted avg       0.71      0.74      0.71       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "print(\"\\t\\t\\t Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "\n",
    "print(\"\\t\\t\\t Classification report\")\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **precision** of the polynomial SVM model is **85%** and the **accuracy** is **83%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Confusion Matrix\n",
      "[[44  0  0  0  1  0  0  0  0]\n",
      " [ 0 95  0  1  1  0  0  0  0]\n",
      " [ 0  0 20  0  1  0  0  0  0]\n",
      " [ 0  1  0 91  1  0  0  0  0]\n",
      " [ 0  1  0  0 81  0  0  0  1]\n",
      " [ 0  0  0  0  1 13  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  0  0  0 11  0  0  9  0]\n",
      " [ 0  0  0  0  5  0  0  0 84]]\n",
      "\n",
      "\n",
      "\t\t\t Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      0.98      0.99        45\n",
      "   building        0.98      0.98      0.98        97\n",
      "        car        1.00      0.95      0.98        21\n",
      "   concrete        0.99      0.98      0.98        93\n",
      "      grass        0.79      0.98      0.88        83\n",
      "       pool        1.00      0.93      0.96        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      0.45      0.62        20\n",
      "       tree        0.99      0.94      0.97        89\n",
      "\n",
      "    accuracy                           0.95       507\n",
      "   macro avg       0.97      0.91      0.93       507\n",
      "weighted avg       0.96      0.95      0.95       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predicting the classes based on train data\n",
    "y_train_pred = svc_best.predict(X_train_scaled)\n",
    "\n",
    "#confusion matrix\n",
    "print(\"\\t\\t\\t Confusion Matrix\")\n",
    "print(confusion_matrix(y_train,y_train_pred))\n",
    "print('\\n')\n",
    "\n",
    "print(\"\\t\\t\\t Classification report\")\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initial glance of the classification it may look like the model is not over fitting, however, if we observe some values across some labels we can see that there's enough evidence to label this model to be **overfitting**.\n",
    "\n",
    "One of the conditions for overfitting is that the train error is too large than the test error. In this case we can compare the accuracy values of the test and train dataset to determine the overfit. The train set accuracy us 95% and test set accuracy is 83%. The train accuracy is significantly greater than train accuracy. Based on these values we can conclude that the Polynomial SVM classifier with GridSearchCV is **overfitting**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Support Vector Machine Classifier + RBF Kernel + Grid Search:\n",
    "\n",
    "We will now use GridSearchCV to try various hyperparameters in a SVM with a RBF kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Use SVC from sklearn with kernel = \"rbf\". Run the GridSearchCV using the following:\n",
    "\n",
    "C: 0.01 - 10 in increments of 0.2  \n",
    "gamma: 0.01,  0.1, 1, 10, 100\n",
    "\n",
    "Note: Feel free to try out more parameters, the above is the bare minimum for this assignment.\n",
    "\n",
    "Use 5 cross-fold and the default scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': np.arange(0.01, 10, 0.2).tolist(),\n",
    "              'kernel': ['rbf'],\n",
    "             'gamma' : [0.01, 0.1, 1, 10, 100]} \n",
    "\n",
    "# create AdaBoost Classifier model \n",
    "svc = svm.SVC()\n",
    "\n",
    "# Use 5 cross-fold and for scoring use \"roc_auc\" \n",
    "svc_Grid = GridSearchCV(svc, param_grid, cv = 5,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.01, 0.21000000000000002, 0.41000000000000003,\n",
       "                               0.6100000000000001, 0.81, 1.01,\n",
       "                               1.2100000000000002, 1.4100000000000001, 1.61,\n",
       "                               1.81, 2.01, 2.21, 2.41, 2.61, 2.81, 3.01, 3.21,\n",
       "                               3.41, 3.61, 3.81, 4.01, 4.21, 4.41, 4.61,\n",
       "                               4.8100000000000005, 5.01, 5.21, 5.41, 5.61,\n",
       "                               5.8100000000000005, ...],\n",
       "                         'gamma': [0.01, 0.1, 1, 10, 100], 'kernel': ['rbf']})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit our model to our train data\n",
    "svc_Grid.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### b) Identify the best performing model:\n",
    "\n",
    ".best_params_() : This method outputs to best performing parameters\n",
    ".best_estimator_() : This method outputs the best performing model, and can be used for predicting on the X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2.81, 'gamma': 0.01, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best performing parameters\n",
    "svc_Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2.81, gamma=0.01)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best performing model\n",
    "svc_Grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Use the best estimator model to predict on test data. Use the .predict() method to get the predicted classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the best estimator model\n",
    "svc_best = svm.SVC(C=2.81, gamma =0.01,kernel='rbf')\n",
    "\n",
    "#Fitting the best fit model on train data\n",
    "svc_best.fit(X_train_scaled,y_train)\n",
    "\n",
    "#predicting the classes of test data\n",
    "y_pred = svc_best.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Calculate the confusion matrix and classification report for test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Confusion Matrix\n",
      "[[13  0  0  0  0  0  1  0  0]\n",
      " [ 0 19  0  5  1  0  0  0  0]\n",
      " [ 0  0 14  1  0  0  0  0  0]\n",
      " [ 0  3  0 20  0  0  0  0  0]\n",
      " [ 0  1  0  0 24  0  0  0  4]\n",
      " [ 0  0  0  0  0 14  1  0  0]\n",
      " [ 1  0  0  1  0  0 14  0  0]\n",
      " [ 0  1  0  5  3  0  0  5  0]\n",
      " [ 0  0  0  1  0  0  0  0 16]]\n",
      "\n",
      "\n",
      "\t\t\t Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        0.93      0.93      0.93        14\n",
      "   building        0.79      0.76      0.78        25\n",
      "        car        1.00      0.93      0.97        15\n",
      "   concrete        0.61      0.87      0.71        23\n",
      "      grass        0.86      0.83      0.84        29\n",
      "       pool        1.00      0.93      0.97        15\n",
      "     shadow        0.88      0.88      0.88        16\n",
      "       soil        1.00      0.36      0.53        14\n",
      "       tree        0.80      0.94      0.86        17\n",
      "\n",
      "    accuracy                           0.83       168\n",
      "   macro avg       0.87      0.83      0.83       168\n",
      "weighted avg       0.85      0.83      0.82       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "print(\"\\t\\t\\t Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "\n",
    "print(\"\\t\\t\\t Classification report\")\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **precision** of the polynomial SVM model is **83%** and the **accuracy** is **83%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### e)  Calculate predictions for the training data & build the classification report & confusion matrix. Are there signs of overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Confusion Matrix\n",
      "[[45  0  0  0  0  0  0  0  0]\n",
      " [ 0 96  0  1  0  0  0  0  0]\n",
      " [ 0  0 21  0  0  0  0  0  0]\n",
      " [ 0  1  0 92  0  0  0  0  0]\n",
      " [ 0  1  0  0 81  0  0  0  1]\n",
      " [ 0  0  0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0]\n",
      " [ 0  1  0  0  0  0  0 19  0]\n",
      " [ 0  0  0  0  1  0  0  0 88]]\n",
      "\n",
      "\n",
      "\t\t\t Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    asphalt        1.00      1.00      1.00        45\n",
      "   building        0.97      0.99      0.98        97\n",
      "        car        1.00      1.00      1.00        21\n",
      "   concrete        0.99      0.99      0.99        93\n",
      "      grass        0.99      0.98      0.98        83\n",
      "       pool        1.00      1.00      1.00        14\n",
      "     shadow        1.00      1.00      1.00        45\n",
      "       soil        1.00      0.95      0.97        20\n",
      "       tree        0.99      0.99      0.99        89\n",
      "\n",
      "    accuracy                           0.99       507\n",
      "   macro avg       0.99      0.99      0.99       507\n",
      "weighted avg       0.99      0.99      0.99       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predicting the classes based on train data\n",
    "y_train_pred = svc_best.predict(X_train_scaled)\n",
    "\n",
    "#confusion matrix\n",
    "print(\"\\t\\t\\t Confusion Matrix\")\n",
    "print(confusion_matrix(y_train,y_train_pred))\n",
    "print('\\n')\n",
    "\n",
    "print(\"\\t\\t\\t Classification report\")\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification report of the train data we can observe that the model has generalized the data to the maximum extent and was able to predict the train data with **100% accuracy**. So, based on the above metrics and the test metrics we can conclude that the random forest with default parameters is **overfitting**\n",
    "\n",
    "One of the conditions for overfitting is that the train error is too large than the test error. In this case we can compare the accuracy values of the test and train dataset to determine the overfit. The train set accuracy us 99% and test set accuracy is 83%. The train accuracy is significantly greater than train accuracy. Based on these values we can conclude that the Polynomial SVM classifier with GridSearchCV is **overfitting**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conceptual Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) From the models run in steps 2-6, which performs the best based on the Classification Report? Support your reasoning with evidence around your test data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classifciation report of the test data and train data, **Support Vector Machine Classifier + Linear Kernel + Grid Search performed the best**.The other models displayed signs of overfitting.\n",
    "\n",
    "With SVC Linear Kernel, there are no signs of overfitting, and the accuracy for the predictions on the test set is 83%, and the difference in the accuracy scores for Linear Kernel of the train and test set is the minimum when compared to other models. On other hand, accuracy score of train data is 90%. There isn't a significant difference in the accuracy scores.\n",
    "\n",
    "The classification report for the SVC Linear kernel model prediction for test data.\n",
    "\n",
    "<img src=\"Screen Shot 2020-11-05 at 5.49.33 PM.png\">\n",
    "\n",
    "The classification report for the SVC Linear kerne model prediction for train data.\n",
    "\n",
    "<img src=\"Screen Shot 2020-11-05 at 5.49.38 PM.png\">\n",
    "\n",
    "By comparing the both classifications report, we can observe that the model performed well on both train and test data.\n",
    "\n",
    "The generalization error decreases and the training error increases.\n",
    "\n",
    "Based on the above mentioned points, we can conclude that the model **does'nt overfit** and is the best performing model compared to the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Compare models run for steps 4-6 where different kernels were used. What is the benefit of using a polynomial or rbf kernel over a linear kernel? What could be a downside of using a polynomial or rbf kernel? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM Linear kernel performs the best based on the Classification report of different models run in steps 4-6. Other models have displayed signs of overfitting whereas there weren't any indications of overfitting in the SVM Linear kernel\n",
    "\n",
    "- The polynomial kernel displayed signs of overfitting. The train accuracy is significantly greater than the test accuracy (the difference in accuracy scores is round 16%). Based on these scores we've concluded the model to be **overfitting**\n",
    "\n",
    "- Similarly, the rbf kernel displayed signs of overfitting too.The train accuracy is significantly greater than the test accuracy (the difference in accuracy scores is round 15%). Based on these scores we've concluded the model to be **overfitting**\n",
    "\n",
    "The linear, polynomial and RBF kernel are different cases of introducing the hyperplane decision boundary between the classes.\n",
    "\n",
    "Polynomial or RBF kernels are basically used when the data is not linear and we cannot use a linear seperable boundary to classify different classes of the data. Polynomial and RBF kernels are generally used when we deal with multi-dimensional data like audio, video or image data. These types of data are non-linear and it's difficult to classify the data using a Linear kernel. Also, linear kernel does not project the data into higher dimensional space, its just the inner product of x and y in the lower dimensional space.\n",
    "\n",
    "The disadvantage of using a polynomial or a rbf kernel compared to a Linear SVM is that, linear kernel is a parametric model, and RBF or polynomial kernel SVM isn't, and the complexity of the latter grows with the size of the training set. Also, training of an RBF and polynomial kernel is very expensive compared to the linear kernel.Complex models tend to ovefit the data compared to the simpler ones.We've observed the same in the above exmaples too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Explain the 'C' parameter used in steps 4-6. What does a small C mean versus a large C in sklearn? Why is it important to use the 'C' parameter when fitting a model? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'C' paramteter used in steps 4-6 is a regularization parameter, it indicates the SVM optimization about the how degree to which we want to avoid misclassifying each training example.\n",
    "\n",
    "Generally while building a SVM model we search for two things: a hyperplane with the largest minimum margin, and a hyperplane that correctly separates as many instances as possible. The problem is that we'll not always be able to achieve both things. The **C** parameter determines degree to which we want to avoid misclassifying. Below are the representation of the same.\n",
    "\n",
    "<img src=\"Screen Shot 2020-11-05 at 6.40.24 PM.png\">\n",
    "\n",
    "\n",
    "To the left is the representation of a low C value, this indicates a pretty large minimum margin (purple). However, this requires that we neglect the blue circle outlier that we have failed to classify correct. On the right we have a high C. In this case we are classifying all the data correctly and thus end up with a much smaller margin.\n",
    "\n",
    "But how do we determine the value of C? It depends on the data. If the data looks something like below plot, then high value of C is best for the model.\n",
    "\n",
    "<img src=\"Screen Shot 2020-11-05 at 6.45.04 PM.png\">\n",
    "\n",
    "If the data looks something like below plot, then low value of C is best for the model.\n",
    "\n",
    "<img src=\"Screen Shot 2020-11-05 at 6.45.40 PM.png\">\n",
    "\n",
    "To summarize,\n",
    " - A high value for C allows the algorithm to classify all of the data correctly rather than leaving wiggle room for future data\n",
    " - A very small value of C will cause the optimizer to look for a larger margin separating hyperplane, even if that hyperplane misclassifies more points\n",
    " \n",
    "Large values for the C parameter tend to overfit the data. Its suggested to run a GridSearchCV on the C parameter to identify the optimumvalue for C in order to avoid overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Scaling our input data does not matter much for Random Forest, but it is a critical step for Support Vector Machines. Explain why this is such a critical step. Also, provide an example of a feature from this data set that could cause issues with our SVMs if not scaled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM) optimization occurs by minimizing the decision vector w, the optimal hyperplane is influenced by the scale of the input features and itâ€™s therefore recommended that data be standardized (mean 0, var 1) prior to SVM model training.\n",
    "\n",
    "The non-standardized data produces decision hyperplanes that are highly sensitive to coefficient C. The standardized data produces much more consistent SVM hyperplanes across hyperparameters.\n",
    "\n",
    "SVM tries to maximize the distance between the separating plane and the support vectors. If any of the features have large values then it will dominate the other features while calculating the distance. If we rescale all features (e.g. to [0, 1]), they all have the same influence on the distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Describe conceptually what the purpose of a kernel is for Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine is a supervised learning algorithm mostly used for classification and regression. The main idea is that based on the training data the algorithm tries to find the optimal hyperplane which can be used to classify new data points. In two dimensions the hyperplane is a simple line.\n",
    "\n",
    "The SVM works by finding the most similar examples between classes. Those will be the support vectors.Based on these support vectors, the algorithm tries to find the best hyperplane that separates the classes. If we cannot find a straight line to separate the labels we will use the Kernel Trick!\n",
    "\n",
    "Kernel Function is a method that transforms input data into the required form of processing data. â€œKernelâ€ is used due to set of mathematical functions used in Support Vector Machine provides the window to manipulate the data. So, Kernel Function generally transforms the training set of data so that a non-linear decision surface is able to transformed to a linear equation in a higher number of dimension spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
