{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The train & test data is pretty clean in terms of image data, but we will need to do a bit of prep work to use in our model. \n",
    "a) Use the \"ImageDataGenerator()\" class from keras.processing.image to build out an instance called \"train_datagen\" with the following parameters: \n",
    "\n",
    "rescale = 1./255  \n",
    "shear_range = 0.2  \n",
    "zoom_range = 0.2  \n",
    "horizontal_flip = True  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.image_data_generator.ImageDataGenerator at 0x7fb2f1976f10>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = keras_preprocessing.image.ImageDataGenerator(rescale=1/255, shear_range=0.2, \n",
    "                                                             zoom_range= 0.2, horizontal_flip=True)\n",
    "train_datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Then build your training set by using the method \".flow_from_directory()\"\n",
    "\n",
    "path (where training data is stored)  \n",
    "target_size = (64, 64)  \n",
    "batch_size = 32  \n",
    "class_mode = categorical   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.directory_iterator.DirectoryIterator at 0x7fb2f19c9190>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset_train',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "train_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Take a look at your training set: \n",
    "\n",
    "What is the image shape of each training observation?  \n",
    "How many total classes do we need to predict on? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of the image\n",
    "n_shape = train_generator.image_shape\n",
    "n_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classes of the image\n",
    "n_classes = np.unique(train_generator.classes)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Classifier Build: \n",
    "\n",
    "Now use keras to build an initial image classifier with the following specifications.\n",
    "\n",
    "Note: If you get lost, there is great documentation online and homework 7 included details on many of the layers used here.\n",
    "\n",
    "- Create an instance of Sequential called \"classifier\"  \n",
    "- Add a Conv2D layer with the following parameters:   \n",
    "    - filters = 32  \n",
    "    - kernel_size = (3,3)  \n",
    "    - input_shape = image shape found in part 1  \n",
    "    - activation = relu  \n",
    "- Add a MaxPooling2D layer where pool_size = (2,2)  \n",
    "- Add another Conv2D layer:   \n",
    "    - filters = 64  \n",
    "    - kernel_size = (3,3)  \n",
    "    - activation = relu  \n",
    "- Add a MaxPooling2D layer where pool_size = (2,2)  \n",
    "- Add a Flatten layer  \n",
    "- Add a Dense layer  \n",
    "    - units = 128  \n",
    "    - activation = relu  \n",
    "- Add a final Dense layer (this will output our probabilities):  \n",
    "    - units = # of classes  \n",
    "    - activation = softmax   \n",
    "- Compile with the following:   \n",
    "    - optimize = adam  \n",
    "    - loss = categorical cross entropy  \n",
    "    - metric = accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier() : \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=n_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(units = 4, activation='softmax'))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN model\n",
    "model1 = classifier()\n",
    "\n",
    "# compile the model\n",
    "model1.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,625,668\n",
      "Trainable params: 1,625,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Runs: \n",
    "This will be run various times with different numbers of steps_per_epoch and epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### a) Use .fit() with the training set. For the first run, use the following parameters: \n",
    "    - steps_per_epoch = 3\n",
    "    - epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3/3 [==============================] - 3s 945ms/step - loss: 1.3745 - acc: 0.3163\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.7247 - acc: 0.8191\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 1s 191ms/step - loss: 0.3588 - acc: 0.9254\n"
     ]
    }
   ],
   "source": [
    "my_model = model1.fit_generator(train_generator,steps_per_epoch=3, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) save model to a file. An example is below:\n",
    "save model  \n",
    "classifier.save('my_model.h5')  \n",
    "print(\"Saved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model1.save('my_model.h5')\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Predict using the model built in step 2. An example below shows how to load a model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "dataset_test/C033.png\n",
      "dataset_test/1022.png\n",
      "dataset_test/4011.png\n",
      "dataset_test/1053.png\n",
      "dataset_test/6051.png\n",
      "dataset_test/4053.png\n",
      "dataset_test/C014.png\n",
      "dataset_test/6023.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([3]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([3]),\n",
       " array([1])]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# test data path\n",
    "img_dir = \"dataset_test\" # Enter Directory of test set\n",
    "\n",
    "# iterate over each test image\n",
    "data_path = os.path.join(img_dir, '*g')\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "# print the files in the dataset_test folder \n",
    "for f in files:\n",
    "    print(f)\n",
    "    \n",
    "# make a prediction and add to results \n",
    "data = []\n",
    "results = []\n",
    "for f1 in files:\n",
    "    img = image.load_img(f1, target_size = (64, 64))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    data.append(img)\n",
    "    result = model.predict(img)\n",
    "    r = np.argmax(result, axis=1)\n",
    "    results.append(r)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Determine accuracy.\n",
    "- Look into the training data(images) in the dataset_train folder, and then determine how a category was coded in keras using the following code:\n",
    "    - check category labels in training_set\n",
    "    - training_set.class_indices\n",
    "- look in the test data(images) in the dataset_test folder, and identify what category each images belongs to using images in the training set as references(there are only 8 test observations).\n",
    "- Create a list to store the category/labels for the test data as the actual values. \n",
    "    - For example, test_label= [3, 0, 2, 0, 1, 2, 3, 1]\n",
    "- Compare the predicted values to the actual values for the test set and calculate accuracy score\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category 1': 0, 'category 2': 1, 'category 3': 2, 'category 4': 3}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check category labels in training_set\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_test/C033.png',\n",
       " 'dataset_test/1022.png',\n",
       " 'dataset_test/4011.png',\n",
       " 'dataset_test/1053.png',\n",
       " 'dataset_test/6051.png',\n",
       " 'dataset_test/4053.png',\n",
       " 'dataset_test/C014.png',\n",
       " 'dataset_test/6023.png']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#idenitifying the labels of the test data\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above test data the test labels are  **test_label = [3,0,2,0,1,2,3,1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = [3,0,2,0,1,2,3,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since accuracy is the ratio of the  measure the number of correct decisions your classifier makes, and the total number of test examples, and the result is the accuracy of your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 75.0 %\n"
     ]
    }
   ],
   "source": [
    "#calculating accuracy by comparing the predicted values and test_labels\n",
    "accuracy = (6/8)*100\n",
    "print('accuracy:', accuracy,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Run this process for the following combinations:\n",
    "\n",
    "* (steps_per_epoch: 1, epochs: 1)\n",
    "* (steps_per_epoch: 1, epochs: 2)\n",
    "* (steps_per_epoch: 1, epochs: 3)\n",
    "* (steps_per_epoch: 2, epochs: 4)\n",
    "* (steps_per_epoch: 2, epochs: 5)\n",
    "* (steps_per_epoch: 2, epochs: 6)\n",
    "* (steps_per_epoch: 3, epochs: 7)\n",
    "* (steps_per_epoch: 3, epochs: 8)\n",
    "* (steps_per_epoch: 5, epochs: 9)\n",
    "* (steps_per_epoch: 5, epochs: 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.4374 - acc: 0.2500\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4343 - acc: 0.0833\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.3877 - acc: 0.1875\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5012 - acc: 0.1875\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7711 - acc: 0.1875\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.0379 - acc: 0.1250\n",
      "Epoch 1/4\n",
      "2/2 [==============================] - 3s 1s/step - loss: 1.8129 - acc: 0.3447\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.3837 - acc: 0.4568\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.8452 - acc: 0.8438\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 0.8235 - acc: 0.8482\n",
      "Epoch 1/5\n",
      "2/2 [==============================] - 3s 1s/step - loss: 1.8926 - acc: 0.1905\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.1138 - acc: 0.4812\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.9112 - acc: 0.6719\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.7108 - acc: 0.8681\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 0.4648 - acc: 0.9112\n",
      "Epoch 1/6\n",
      "2/2 [==============================] - 3s 1s/step - loss: 1.6216 - acc: 0.2629\n",
      "Epoch 2/6\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.0410 - acc: 0.6094\n",
      "Epoch 3/6\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.7322 - acc: 0.7128\n",
      "Epoch 4/6\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4645 - acc: 0.9311\n",
      "Epoch 5/6\n",
      "2/2 [==============================] - 1s 332ms/step - loss: 0.2961 - acc: 0.9219\n",
      "Epoch 6/6\n",
      "2/2 [==============================] - 1s 376ms/step - loss: 0.2296 - acc: 0.9276\n",
      "Epoch 1/7\n",
      "3/3 [==============================] - 3s 1s/step - loss: 2.0222 - acc: 0.4309\n",
      "Epoch 2/7\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 1.1466 - acc: 0.6094\n",
      "Epoch 3/7\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.7298 - acc: 0.7955\n",
      "Epoch 4/7\n",
      "3/3 [==============================] - 1s 384ms/step - loss: 0.3420 - acc: 0.9361\n",
      "Epoch 5/7\n",
      "3/3 [==============================] - 1s 393ms/step - loss: 0.3048 - acc: 0.9283\n",
      "Epoch 6/7\n",
      "3/3 [==============================] - 1s 390ms/step - loss: 0.2601 - acc: 0.9202\n",
      "Epoch 7/7\n",
      "3/3 [==============================] - 1s 402ms/step - loss: 0.1493 - acc: 0.9548\n",
      "Epoch 1/8\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.4521 - acc: 0.3402\n",
      "Epoch 2/8\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 1.0490 - acc: 0.7045\n",
      "Epoch 3/8\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.5715 - acc: 0.8830\n",
      "Epoch 4/8\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.3245 - acc: 0.9096\n",
      "Epoch 5/8\n",
      "3/3 [==============================] - 1s 439ms/step - loss: 0.2436 - acc: 0.9415\n",
      "Epoch 6/8\n",
      "3/3 [==============================] - 1s 373ms/step - loss: 0.1767 - acc: 0.9283\n",
      "Epoch 7/8\n",
      "3/3 [==============================] - 1s 404ms/step - loss: 0.1000 - acc: 0.9467\n",
      "Epoch 8/8\n",
      "3/3 [==============================] - 1s 394ms/step - loss: 0.1181 - acc: 0.9680\n",
      "Epoch 1/9\n",
      "5/5 [==============================] - 4s 713ms/step - loss: 1.3134 - acc: 0.4172\n",
      "Epoch 2/9\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.5794 - acc: 0.9146\n",
      "Epoch 3/9\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.2243 - acc: 0.9537\n",
      "Epoch 4/9\n",
      "5/5 [==============================] - 2s 365ms/step - loss: 0.1291 - acc: 0.9601\n",
      "Epoch 5/9\n",
      "5/5 [==============================] - 2s 408ms/step - loss: 0.0879 - acc: 0.9667\n",
      "Epoch 6/9\n",
      "5/5 [==============================] - 2s 407ms/step - loss: 0.0319 - acc: 1.0000\n",
      "Epoch 7/9\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.0647 - acc: 0.9728\n",
      "Epoch 8/9\n",
      "5/5 [==============================] - 2s 439ms/step - loss: 0.0447 - acc: 0.9793\n",
      "Epoch 9/9\n",
      "5/5 [==============================] - 2s 464ms/step - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 4s 736ms/step - loss: 1.4637 - acc: 0.3461\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.6894 - acc: 0.8825\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.2712 - acc: 0.9433\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.2421 - acc: 0.9392\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.1286 - acc: 0.9647\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 2s 393ms/step - loss: 0.1043 - acc: 0.9559\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 2s 369ms/step - loss: 0.0647 - acc: 0.9855\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 2s 376ms/step - loss: 0.0539 - acc: 0.9774\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.0309 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.0310 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#iteration proccess for creating different models for various steps_per_epoch and epochs\n",
    "steps_per_epoch = [1,1,1,2,2,2,3,3,5,5]\n",
    "epochs = [1,2,3,4,5,6,7,8,9,10]\n",
    "score=[]\n",
    "\n",
    "for i in range(0,len(steps_per_epoch)):\n",
    "    model1 = classifier()\n",
    "    # compile the model\n",
    "    model1.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    model1.fit_generator(train_generator,steps_per_epoch=steps_per_epoch[i], epochs=epochs[i])\n",
    "    model_name = \"model_\" +str(steps_per_epoch[i])+\"_\"+str(epochs[i])\n",
    "    model1.save(model_name)\n",
    "    \n",
    "    model = load_model(model_name)\n",
    "# make a prediction and add to results \n",
    "    data = []\n",
    "    results = []\n",
    "    for f1 in files:\n",
    "        img = image.load_img(f1, target_size = (64, 64))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis = 0)\n",
    "        data.append(img)\n",
    "        result = model.predict(img)\n",
    "        r = np.argmax(result, axis=1)\n",
    "        results.append(r)\n",
    "\n",
    "    results = list(np.concatenate(results))\n",
    "    \n",
    "    score.append([steps_per_epoch[i], epochs[i], results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Create a final dataframe that combines the accuracy across each combination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 2, 0, 1, 2, 3, 1]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels of the test data\n",
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = []\n",
    "for i in range(0,len(steps_per_epoch)):\n",
    "    accuracy=0\n",
    "    for j in range(0,len(test_label)):\n",
    "        if score[i][2][j] == test_label[j]:\n",
    "            accuracy += 1\n",
    "        \n",
    "    accuracy_scores.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps_per_epoch</th>\n",
       "      <th>epoch</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>87.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>87.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   steps_per_epoch  epoch  accuracy\n",
       "0                1      1      50.0\n",
       "1                1      2      37.5\n",
       "2                1      3      25.0\n",
       "3                2      4      75.0\n",
       "4                2      5      87.5\n",
       "5                2      6      75.0\n",
       "6                3      7      75.0\n",
       "7                3      8      87.5\n",
       "8                5      9      75.0\n",
       "9                5     10      75.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'steps_per_epoch' : steps_per_epoch, 'epoch': epochs,'accuracy' : accuracy_scores}\n",
    "accuracy_comp = pd.DataFrame(data)\n",
    "accuracy_comp['accuracy'] = (accuracy_comp['accuracy']/8)*100\n",
    "accuracy_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Questions: \n",
    "\n",
    "#### 4. Discuss the effect of the following on accuracy and loss (train & test): \n",
    "- Increasing the steps_per_epoch\n",
    "- Increasing the number of epochs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the steps_per_epoch and number_of_epochs increases the accuracy of both the train set and test increases, however, over a point of time the network starts to overfit. We can observe the same in the above iterations.\n",
    "For example, consider the iteration steps_per_epoch=5 and number_of_epochs=10. We can observe that the train accuracy is increasing and after a point the network starts to overfit. And similarly the test accuracy tends to increase and then decrease.\n",
    "\n",
    "As for the loss we can observe that the loss decreases gradually as we keep increasing the steps_per_epoch and the corresponding epochs.\n",
    "\n",
    "<img src = \"Screen Shot 2020-11-19 at 10.31.57 PM.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Name two uses of zero padding in CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zero padding allows modification of the size of the input to be adjusted to our requirement. It is mostly used in designing the CNN layers when the dimensions of the input volume need to be preserved in the output volume\n",
    "- Zero padding can also be used to control the shrinkage of dimension after applying filters larger than 1x1\n",
    "- Can also be used to be avoid loosing information at the boundaries, e.g. when weights in a filter drop rapidly away from its center\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What is the use of a 1 x 1 kernel in CNN? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 1×1 kernel can be used to create a linear projection of a stack of feature maps\n",
    "- The projection created by a 1×1 can act like channel-wise pooling and be used for dimensionality reduction\n",
    "- The projection created by a 1×1 can also be used directly or be used to increase the number of feature maps in a model\n",
    "- Create deeper network through “Bottle-Neck” layer\n",
    "- Create smaller CNN network which retains higher degree of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What are the advantages of a CNN over a fully connected DNN for this image classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional layer is much more specialized, and efficient, than a fully connected layer.\n",
    "\n",
    "In a fully connected layer each neuron is connected to every neuron in the previous layer, and each connection has it's own weight. This is a totally general purpose connection pattern and makes no assumptions about the features in the data. It's also very expensive in terms of memory (weights) and computation.\n",
    "\n",
    "In contrast, in a convolutional layer each neuron is only connected to a few nearby neurons in the previous layer, and the same set of weights is used for every neuron. This connection pattern only makes sense for cases where the data can be interpreted as spatial with the locally extrated features and equally likely to occur at any input position. The typical use case for convolutional layers is for image data where, as required, the features are local (e.g. a \"nose\" consists of a set of nearby pixels, not spread all across the image), and equally likely to occur anywhere (in general case, that nose might be anywhere in the image).\n",
    "\n",
    "In the current problem, CNN is prefferd over the fully connected neural network due to the data being images. Images are made up of large number of pixels. The fewer number of connections and weights make convolutional layers relatively cheap vs full connect, in terms of memory and compute power needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
